---
title: "Scraping BBC Food Recipes"
author: "Dominik Gulacsy"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---

```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```
```{r setup, include=FALSE}
suppressWarnings(suppressMessages(library(knitr)))
path <- "/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/"
```

## Intro

I used [**BBC Food**](https://www.bbc.co.uk/food) as a target site for this webscraping project. I opted for this site because it's content for me was relevant. Actually I am interested in creating a recipe gatherer and an app that can easily show the steps of the cooking on a tablet, can make recommendations on dietary and scheduling preferences (e.g: gluten-free, batch cooking) and has the feature to show recipe suggestion when uploading a picture of ingredients by calculating ingredient similarity scores. With a built-in ingredient stock (resource) manager which predicts based on consumption habits when certain ingredients will get low. Regarding technicalities the basic structure of site was easy to understand and made the scraping process relatively clear. Yet there was some JSONs that could be extracted to make the process much more manageable and long-life.

## Step-by-step description of my webscraping script

Note: I used the following libraries to write the script:
```{r echo=FALSE}
suppressWarnings(suppressMessages(library(rvest)))
suppressWarnings(suppressMessages(library(jsonlite)))
suppressWarnings(suppressMessages(library(data.table)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(tibble)))
suppressWarnings(suppressMessages(library(purrr)))
suppressWarnings(suppressMessages(library(ggthemes)))
```
```
library(rvest)
library(selectr)
library(prettydoc)
library(dplyr)
library(data.table)
```

1. Overview
While investigating the site's structure I recognized that recipes are organized on the site in the following way:
1. A-Z indexing
"https://www.bbc.co.uk/food/recipes/a-z/a/1#featured-content"
![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/a-z.JPG)
2. Pagination
"https://www.bbc.co.uk/food/recipes/a-z/b/7#featured-content"
![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/pagination.JPG)
3. Recipe sites
"https://www.bbc.co.uk/food/recipes/aclassicspongecakewi_9406"
![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/recipe_sites.JPG)


This structure represents the core logic behind my script. So firstly, I should get the initial letter pages, then the paginated pages an lastly all the recipe pages listed on the before mentioned pages. 

2. Getting the JSONs
Going through the Network tab in Google Chrome I found that all of these page types contain a JSON in their HTML doc. 

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/json.JPG)

However, getting this JSON is not that easy it may look since there is no id or class associated with the script tag that contains the JSON. So in order to properly extract these JSONs I had to use a function that can look for the ```window.__reactInitialState__ =``` expression among the script tags and return the JSON of the one that includes it.

```{r eval=FALSE}
# Create function to extract JSON from the HTML Doc
get_json_values <- function(my_page , my_look_for, ful_name) {
  #my_look_for <- 'reactInitialState'
  #ful_name <- 'window.__reactInitialState__ = '
  script_list <- 
    my_page %>%
    html_nodes("script")
  
  for (i in script_list) {
    my_t <- i %>% 
      html_text()
    my_t_h <- substring(my_t, 1,100)
    #print(my_t)
    if(grepl(my_look_for, my_t_h)){
      res <- substring(str_trim(my_t), nchar(ful_name)+1)
      res <- gsub('\\</b>', '',gsub('\\<b>', '', gsub('}};', '}}', res, fixed = T), fixed = T), fixed = T)
      return(fromJSON(res))
    }
  }
}
```

We can get the necessary data this way on every level. Actually, I needed to get the one JSON to know what are the initial letter indexes and get the JSON for every page and every recipe.

To further develop my script I retrieve the JSON from "https://www.bbc.co.uk/food/recipes/a-z/a/1#featured-content" which contains the data on initial letters and the URLs for the recipe pages in azPageReducer item.

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/json_letters.JPG)

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/json_urls.JPG)

Then I also extracted the JSON from "https://www.bbc.co.uk/food/recipes/aclassicspongecakewi_9406" just to see what kind of data can be collected from the page. I decided to collect the recipe category, title, description, some diet labels, collection label, chef, preparation time, cook time, food name, number of ratings and rating score. Plus the ingredients, stages, instructions and occasion labels as lists too.

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/pics/json_recipes.JPG)

3. Getting the ingredients from the JSON
Getting the ingredients from the JSON was not easy as they were embedded in the stage items. So I needed to consolidate all ingredients from all stages of the recipe to have the whole list of ingredients.

```{r eval=FALSE}
# Create function to get ingredients from nested lists
get_ingredients <- function() {
  stages<-recipe_json$recipeReducer$recipe$stagesWithoutLinks
  ing <- c()
  for (stage in 1:length(stages)){
    for (aux in 1:length(stages[stage,"ingredients"])){
      for (ingredient in 1:length(stages[stage,"ingredients"][[aux]]$foods)){
        ing <- c(ing,stages[stage,"ingredients"][[aux]]$foods[[ingredient]]$title)
      }
    }
  }
  return(ing)
}
```

4. Handling missing values
Later on during the collection of recipes I realized that I needed a mechanism to check if a list was empty and return with NA if so. Thus I wrote the following function and created an other to check if items are being NULL:

```{r eval=FALSE}
# Create function that returns NA when list is empty and with element if not
get_if_not_empty <- function(list,row,col) {
  if (is_empty(list)){
    return(NA)
  } else{
    return(list[row,col]) 
  }
}

# Create function that returns NA when item is null and with itself if not
get_if_not_null <- function(item) { 
  if (is_null(item)) {
    return(NA)
  } else{
    return(item)
  }
}
```

5. Initializing the automatized scraper
To get ready to scrape all recipe pages, I got the initial letter indexes from the above mentioned JSON and saved it into a variable. I created a data frame for the variables intended to be scraped and empty lists for the ingredients, stages, instructions and occasion labels.

```{r eval=FALSE}
# Find BBC Food page to get letter page letters
t <- read_html('https://www.bbc.co.uk/food/recipes/a-z/a/1#featured-content')
write_html(t, paste0(path, "bbc_food_page.html"))
d <- "https://www.bbc.co.uk"

init_json <- get_json_values(t, 'reactInitialState', 'window.__reactInitialState__ = ')
letters<- names(init_json$azPageReducer$azCounts)

# Initialize output table

df<-data.frame(
  "category"= character(),
  "title"= character(),
  "description"= character(),
  "diet1" = character(),
  "diet2" = character(),
  "diet3" = character(),
  "collection" = character(),
  "chef" = character(),
  "prep_time" = character(),
  "cook_time" = character(),
  "food" = character(),
  "rating_no" = numeric(),
  "rating_val" = numeric()
)

stages <- list()
ingredients <- list()
instructions <- list()
occasions <- list()
```

6. Running the Automated Scraper
My script gets the data from the BBC Food site using 3 for loops. This inherently comes from the structure of the site discussed in the overview section. The first loop gets the letter sites by plugging the letters in the URL. The second loop gets the site for every page by plugging in the max page based on the initial letter. Finally the last loop gets all recipe pages that are listed on the pages. The scraper also archives the HTML document in a corresponding folder structure which is created on the fly. Each set of values is added as a row recipe by recipe to the "df" data frame. Eventually the collected list elements are also appended to the data frame as columns.

```{r eval=FALSE, echo=FALSE}
# Iterate through initial letter pages
for (l in letters) {
  print(paste0("Getting recipes with: ",l))
  lpage <- read_html(paste0("https://www.bbc.co.uk/food/recipes/a-z/",l,"/1#featured-content"))
  dir.create(paste0(path,"/htmls/",as.character(l)))
  lpage_json <- get_json_values(lpage, 'reactInitialState', 'window.__reactInitialState__ = ')
  pages<-1:lpage_json$azPageReducer$pages
  print(pages)
  # Iterate through pages
  for (p in pages) {
    print(paste0("Getting recipes on page: ", p))
    dir.create(paste0(path,"/htmls/",l,"/",p))
    page <- read_html(paste0("https://www.bbc.co.uk/food/recipes/a-z/",l,"/",p,"#featured-content"))
    page_json <- get_json_values(page, 'reactInitialState', 'window.__reactInitialState__ = ')
    recipes <- page_json$azPageReducer$promos$url
    print(recipes)
    # Iterate through recipes 
    for (r in recipes) {
      print(paste0("Getting recipe for: ", r))
      r_page <- read_html(paste0(d,r))
      recipe_json <- get_json_values(r_page, 'reactInitialState', 'window.__reactInitialState__ = ')
      print("json ok")
      r_id <- recipe_json$recipeReducer$recipe$id
      write_html(page, paste0(path,"/htmls/",l,"/",p,"/",r_id,".html"))
      df<-df %>% add_row(
        "category"= get_if_not_null(recipe_json$recipeReducer$recipe$course$title),
        "title"= get_if_not_null(recipe_json$recipeReducer$recipe$title),
        "description"= get_if_not_null(recipe_json$recipeReducer$recipe$description),
        "diet1" = get_if_not_empty(recipe_json$recipeReducer$recipe$diets,1,"title"),
        "diet2" = get_if_not_empty(recipe_json$recipeReducer$recipe$diets,2,"title"),
        "collection" = get_if_not_empty(recipe_json$recipeReducer$recipe$collections,1,"title"),
        "chef" = get_if_not_null(recipe_json$recipeReducer$recipe$chefDetails$name),
        "prep_time" = get_if_not_null(recipe_json$recipeReducer$recipe$metadata$prepTimeMeta),
        "cook_time" = get_if_not_null(recipe_json$recipeReducer$recipe$metadata$cookTimeMeta),
        "food" = get_if_not_null(recipe_json$recipeReducer$recipe$food$title),
        "rating_no" = get_if_not_null(recipe_json$recipeReducer$rating$total),
        "rating_val" = get_if_not_null(recipe_json$recipeReducer$rating$value)
      )
      print("non list variables ok")
      stages<-append(stages, list(recipe_json$recipeReducer$recipe$stages$title))
      ingredients<-append(ingredients, list(get_ingredients()))
      instructions<-append(instructions, list(recipe_json$recipeReducer$recipe$methods$text))
      occasions<-append(occasions, list(recipe_json$recipeReducer$recipe$occasions$title))
      print("list variables ok")
    }
  }
}

# Add the lists to the data frame
df$stages <- stages
df$ingredients <- ingredients
df$instructions <- instructions
df$occasions <- occasions
```

7. Adding derived variables
To gain more insight using this data, I also added the following calculated columns: number of stages, number of ingredients, number of instructions, number of occasions. 

```{r eval=FALSE}
df$no_stages <- unlist(lapply(df$stages,length))
df$no_ingredients <- unlist(lapply(df$ingredients,length))
df$no_instructions <- unlist(lapply(df$instructions,length))
df$no_occasions <- unlist(lapply(df$occasions,length))
```

## Explanatory Data Analysis

1. Rating distribution
Firstly, I looked at how the rating of recipes is distributed. It looks like there are very few number of recipes that have more than 50 ratings. So in many cases the true quality of the recipe cannot be evaluated. However, it can be seen that when users indeed rate the recipe they usually rate it good. Most recipes have an above 4 rating.

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/plots/p1.png)

```{r include=FALSE, eval=FALSE}
df[,c("rating_no","rating_val")] %>%
  filter(rating_no!=0) %>% 
  filter(rating_no<200) %>% 
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram()+
  theme_economist() + 
  scale_fill_economist()

summary(df[,c("rating_no","rating_val")])
```

2. Recipe Category distribution
Next I took a look at which recipe categories are the most common. Based on the chart it can be said that most recipes belong to main course category. This is no surprise since it can be considered the broadest category of them all. The following two categories are "cakes and baking" and desserts which combined are still far away from the main course category.

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/plots/p2.png)

```{r include=FALSE, eval=FALSE}
cat_df <- df[,c("category","rating_val","no_stages","no_ingredients","no_instructions")] %>% 
  group_by(category) %>% 
  summarize(
    no_recipes = n(),
    avg_rating_val = mean(rating_val, na.rm = TRUE),
    avg_no_stages = mean(no_stages, na.rm = TRUE),
    avg_no_ingredients = mean(no_ingredients, na.rm = TRUE),
    avg_no_instructions = mean(no_instructions, na.rm = TRUE)
  )

ggplot(cat_df, aes(x = reorder(category, no_recipes), y = no_recipes, fill= no_recipes)) +
  geom_bar(stat="identity")+
  coord_flip() + 
  theme_base() +
  labs(title = 'Recipe categories by frequency',
       y = 'Frequency',
       x = 'Category')
```

3. Most common ingredients
One of the most interesting about this data that I managed to gather all the ingredients needed in every recipe. With this data I could find out which ingredients are the most common in recipes. It turns out that top 5 are black pepper, olive oil, butter, garlic and salt. Salt and pepper are once again not big of a surprise but it looks like that most recipes include some kind of saturated fat which is quite interesting from a dietetical point of view. 

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/plots/p3.png)
```{r include=FALSE, eval=FALSE}
top10 <- data.frame(sort(ing_df, decreasing=T)[1:10],)
colnames(top10) <- c("ingredient", "frequency")

ggplot(top10, aes(x = reorder(ingredient, frequency), y = frequency, fill= frequency)) +
  geom_bar(stat="identity")+
  coord_flip() + 
  theme_base() +
  labs(title = 'Top 10 ingredient by occurance',
       y = 'Frequency',
       x = 'Ingredient')
```

4. Chef performance
Finally, I also investigated whether there is a connection between how chefs' recipes are rated and how complex their average recipe is. It turned out that there is no such relationship between these based on the data. Running a linear regression on the data, the global F test's p value was very large.

![''](/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/assignment2/plots/p4.png)
```{r include=FALSE, eval=FALSE}
chef_df <- df[,c("chef","rating_val","no_ingredients")] %>% 
  group_by(chef) %>% 
  summarize(
    no_recipes = n(),
    avg_rating_val = mean(rating_val, na.rm = TRUE),
    avg_no_ingredients = mean(no_ingredients, na.rm = TRUE)
  ) %>% 
  filter(avg_rating_val!=0)

ggplot(chef_df, aes(x = avg_no_ingredients, y = avg_rating_val)) +
  geom_point( size=2, color="steelblue")+
  theme_base() +
  labs(title = 'Relationship between recipe rating and complexity (avg. # of ingredients)',
       y = 'Average Rating',
       x = 'Average # of ingredients')

reg<-lm(avg_rating_val~avg_no_ingredients, chef_df)
summary(reg)
```

## Summary
To sum up, I scraped all the recipes from BBC Food using an automated scraper which addressed a three-level structure of recipe organization by using nested for loops. I extracted a JSON document from these levels and used the data to navigate the site and retrieve data on recipes. I also conducted some basic explanatory data analysis. I found that recipes are not usually well-rated so it is hard to determine the actual quality of most recipes. However, if recipes are rated then they are rated generally above the 4 score. I also found out that most recipes are in the main course category so this site might be representative of all recipes in existence and it is not a niche site for only sweets for example. My third statement addressed the most common ingredients which eventually were black pepper, olive oil, butter, garlic and salt. And finally I tested whether there was relationship between chef's rating and their average number of recipe ingredients but it turned out that there was no relationship based on the data.
